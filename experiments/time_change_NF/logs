c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_augmented_model_tabular
from ctfp_tools import run_ctfp_model as run_model, parse_arguments, plot_model
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_augmented_model_tabular(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change,
        time_change_dims = args.time_change_dims
    )
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    l2_val_reg_loss_per_epoch = []
    l1_val_reg_loss_per_epoch = []
    l2_train_reg_loss_per_epoch = []
    l1_train_reg_loss_per_epoch = []
    likli_val_loss_per_epoch = []
    num_best_epoch = 0
    start_time = time.time()
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        l2_train_reg_loss = []
        l1_train_reg_loss = []
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss, reg_loss_time_change = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
            if args.l2_reg_loss:
                loss = loss + reg_loss_time_change[0]
            if args.l1_reg_loss:
                loss = loss + reg_loss_time_change[1]
                
            l2_train_reg_loss.append(reg_loss_time_change[0].data.cpu().numpy())
            l1_train_reg_loss.append(reg_loss_time_change[1].data.cpu().numpy())
            
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                reg_loss_message = "Reg_loss l2: %.4f | Reg_loss l1: %.4f"
                print(reg_loss_message %(reg_loss_time_change[0].item(), reg_loss_time_change[1].item()))
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
        # mean l2 regularisation loss
        l2_train_reg_loss_per_epoch.append(np.mean(l2_train_reg_loss))
        l1_train_reg_loss_per_epoch.append(np.mean(l1_train_reg_loss))
        
        if epoch % plot_phi_freq == 0:
            
            time_change_func.show_plot_(times, epoch, args.time_change)
            plot_model(aug_model, times, epoch)
            
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                l2_val_reg_losses = []
                l1_val_reg_losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss, reg_loss_time_change = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                    
                    if args.l2_reg_loss:
                        loss = loss + reg_loss_time_change[0]
                    if args.l1_reg_loss:
                        loss = loss + reg_loss_time_change[1]
                        
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    l2_val_reg_losses.append(reg_loss_time_change[0].data.cpu().numpy())
                    l1_val_reg_losses.append(reg_loss_time_change[1].data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                likli_val_loss_per_epoch.append(loss)
                l2_val_reg_loss_per_epoch.append(np.mean(l2_val_reg_losses))
                l1_val_reg_loss_per_epoch.append(np.mean(l1_val_reg_losses))
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    fig, ax = plt.subplots()
    ax.plot(l2_train_reg_loss_per_epoch, label = "train_reg_loss")
    ax.plot(l2_val_reg_loss_per_epoch, label = "val_reg_loss")
    ax.set_xlabel("epoch")
    ax.set_ylabel("loss")
    ax.set_title("L2 reg loss for func : " + args.time_change)
    leg = ax.legend()
    
    fig, ax = plt.subplots()
    ax.plot(l1_train_reg_loss_per_epoch, label = "train_reg_loss")
    ax.plot(l1_val_reg_loss_per_epoch, label = "val_reg_loss")
    ax.set_xlabel("epoch")
    ax.set_ylabel("loss")
    ax.set_title("L1 reg loss for func : " + args.time_change)
    leg = ax.legend()
    
    fig_name = "validation_loss_" + args.time_change + ".png"
    save_path = os.path.join(args.save, fig_name)
    fig, ax = plt.subplots()
    ax.plot(likli_val_loss_per_epoch)
    ax.set_title("Validation loss : " + args.time_change)
    plt.savefig(save_path, format = "png")
Namespace(data_path='data/gbm_2.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change=None, time_change_dims=None, time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_augmented_model_tabular
from ctfp_tools import run_ctfp_model as run_model, parse_arguments, plot_model
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_augmented_model_tabular(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change,
        time_change_dims = args.time_change_dims
    )
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    l2_val_reg_loss_per_epoch = []
    l1_val_reg_loss_per_epoch = []
    l2_train_reg_loss_per_epoch = []
    l1_train_reg_loss_per_epoch = []
    likli_val_loss_per_epoch = []
    num_best_epoch = 0
    start_time = time.time()
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        l2_train_reg_loss = []
        l1_train_reg_loss = []
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss, reg_loss_time_change = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
            if args.l2_reg_loss:
                loss = loss + reg_loss_time_change[0]
            if args.l1_reg_loss:
                loss = loss + reg_loss_time_change[1]
                
            l2_train_reg_loss.append(reg_loss_time_change[0].data.cpu().numpy())
            l1_train_reg_loss.append(reg_loss_time_change[1].data.cpu().numpy())
            
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                reg_loss_message = "Reg_loss l2: %.4f | Reg_loss l1: %.4f"
                print(reg_loss_message %(reg_loss_time_change[0].item(), reg_loss_time_change[1].item()))
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
        # mean l2 regularisation loss
        l2_train_reg_loss_per_epoch.append(np.mean(l2_train_reg_loss))
        l1_train_reg_loss_per_epoch.append(np.mean(l1_train_reg_loss))
        
        if epoch % plot_phi_freq == 0:
            
            time_change_func.show_plot_(times, epoch, args.time_change)
            plot_model(aug_model, times, epoch)
            
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                l2_val_reg_losses = []
                l1_val_reg_losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss, reg_loss_time_change = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                    
                    if args.l2_reg_loss:
                        loss = loss + reg_loss_time_change[0]
                    if args.l1_reg_loss:
                        loss = loss + reg_loss_time_change[1]
                        
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    l2_val_reg_losses.append(reg_loss_time_change[0].data.cpu().numpy())
                    l1_val_reg_losses.append(reg_loss_time_change[1].data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                likli_val_loss_per_epoch.append(loss)
                l2_val_reg_loss_per_epoch.append(np.mean(l2_val_reg_losses))
                l1_val_reg_loss_per_epoch.append(np.mean(l1_val_reg_losses))
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    fig, ax = plt.subplots()
    ax.plot(l2_train_reg_loss_per_epoch, label = "train_reg_loss")
    ax.plot(l2_val_reg_loss_per_epoch, label = "val_reg_loss")
    ax.set_xlabel("epoch")
    ax.set_ylabel("loss")
    ax.set_title("L2 reg loss for func : " + args.time_change)
    leg = ax.legend()
    
    fig, ax = plt.subplots()
    ax.plot(l1_train_reg_loss_per_epoch, label = "train_reg_loss")
    ax.plot(l1_val_reg_loss_per_epoch, label = "val_reg_loss")
    ax.set_xlabel("epoch")
    ax.set_ylabel("loss")
    ax.set_title("L1 reg loss for func : " + args.time_change)
    leg = ax.legend()
    
    fig_name = "validation_loss_" + args.time_change + ".png"
    save_path = os.path.join(args.save, fig_name)
    fig, ax = plt.subplots()
    ax.plot(likli_val_loss_per_epoch)
    ax.set_title("Validation loss : " + args.time_change)
    plt.savefig(save_path, format = "png")
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_augmented_model_tabular
from ctfp_tools import run_ctfp_model as run_model, parse_arguments, plot_model
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_augmented_model_tabular(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change,
        time_change_dims = args.time_change_dims
    )
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    l2_val_reg_loss_per_epoch = []
    l1_val_reg_loss_per_epoch = []
    l2_train_reg_loss_per_epoch = []
    l1_train_reg_loss_per_epoch = []
    likli_val_loss_per_epoch = []
    num_best_epoch = 0
    start_time = time.time()
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        l2_train_reg_loss = []
        l1_train_reg_loss = []
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss, reg_loss_time_change = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
            if args.l2_reg_loss:
                loss = loss + reg_loss_time_change[0]
            if args.l1_reg_loss:
                loss = loss + reg_loss_time_change[1]
                
            l2_train_reg_loss.append(reg_loss_time_change[0].data.cpu().numpy())
            l1_train_reg_loss.append(reg_loss_time_change[1].data.cpu().numpy())
            
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                reg_loss_message = "Reg_loss l2: %.4f | Reg_loss l1: %.4f"
                print(reg_loss_message %(reg_loss_time_change[0].item(), reg_loss_time_change[1].item()))
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
        # mean l2 regularisation loss
        l2_train_reg_loss_per_epoch.append(np.mean(l2_train_reg_loss))
        l1_train_reg_loss_per_epoch.append(np.mean(l1_train_reg_loss))
        
        if epoch % plot_phi_freq == 0:
            
            time_change_func.show_plot_(times, epoch, args.time_change)
            plot_model(aug_model, times, epoch)
            
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                l2_val_reg_losses = []
                l1_val_reg_losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss, reg_loss_time_change = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                    
                    if args.l2_reg_loss:
                        loss = loss + reg_loss_time_change[0]
                    if args.l1_reg_loss:
                        loss = loss + reg_loss_time_change[1]
                        
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    l2_val_reg_losses.append(reg_loss_time_change[0].data.cpu().numpy())
                    l1_val_reg_losses.append(reg_loss_time_change[1].data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                likli_val_loss_per_epoch.append(loss)
                l2_val_reg_loss_per_epoch.append(np.mean(l2_val_reg_losses))
                l1_val_reg_loss_per_epoch.append(np.mean(l1_val_reg_losses))
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    fig, ax = plt.subplots()
    ax.plot(l2_train_reg_loss_per_epoch, label = "train_reg_loss")
    ax.plot(l2_val_reg_loss_per_epoch, label = "val_reg_loss")
    ax.set_xlabel("epoch")
    ax.set_ylabel("loss")
    ax.set_title("L2 reg loss for func : " + args.time_change)
    leg = ax.legend()
    
    fig, ax = plt.subplots()
    ax.plot(l1_train_reg_loss_per_epoch, label = "train_reg_loss")
    ax.plot(l1_val_reg_loss_per_epoch, label = "val_reg_loss")
    ax.set_xlabel("epoch")
    ax.set_ylabel("loss")
    ax.set_title("L1 reg loss for func : " + args.time_change)
    leg = ax.legend()
    
    fig_name = "validation_loss_" + args.time_change + ".png"
    save_path = os.path.join(args.save, fig_name)
    fig, ax = plt.subplots()
    ax.plot(likli_val_loss_per_epoch)
    ax.set_title("Validation loss : " + args.time_change)
    plt.savefig(save_path, format = "png")
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change=None, time_change_dims=None, time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change=None, time_change_dims=None, time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_augmented_model_tabular
from ctfp_tools import run_ctfp_model as run_model, parse_arguments, plot_model
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_augmented_model_tabular(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change)#,
        #time_change_dims = args.time_change_dims
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    l2_val_reg_loss_per_epoch = []
    l1_val_reg_loss_per_epoch = []
    l2_train_reg_loss_per_epoch = []
    l1_train_reg_loss_per_epoch = []
    likli_val_loss_per_epoch = []
    num_best_epoch = 0
    start_time = time.time()
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        l2_train_reg_loss = []
        l1_train_reg_loss = []
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss, reg_loss_time_change = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
            if args.l2_reg_loss:
                loss = loss + reg_loss_time_change[0]
            if args.l1_reg_loss:
                loss = loss + reg_loss_time_change[1]
                
            l2_train_reg_loss.append(reg_loss_time_change[0].data.cpu().numpy())
            l1_train_reg_loss.append(reg_loss_time_change[1].data.cpu().numpy())
            
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                reg_loss_message = "Reg_loss l2: %.4f | Reg_loss l1: %.4f"
                print(reg_loss_message %(reg_loss_time_change[0].item(), reg_loss_time_change[1].item()))
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
        # mean l2 regularisation loss
        l2_train_reg_loss_per_epoch.append(np.mean(l2_train_reg_loss))
        l1_train_reg_loss_per_epoch.append(np.mean(l1_train_reg_loss))
        
        if epoch % plot_phi_freq == 0:
            
            time_change_func.show_plot_(times, epoch, args.time_change)
            plot_model(aug_model, times, epoch)
            
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                l2_val_reg_losses = []
                l1_val_reg_losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss, reg_loss_time_change = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                    
                    if args.l2_reg_loss:
                        loss = loss + reg_loss_time_change[0]
                    if args.l1_reg_loss:
                        loss = loss + reg_loss_time_change[1]
                        
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    l2_val_reg_losses.append(reg_loss_time_change[0].data.cpu().numpy())
                    l1_val_reg_losses.append(reg_loss_time_change[1].data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                likli_val_loss_per_epoch.append(loss)
                l2_val_reg_loss_per_epoch.append(np.mean(l2_val_reg_losses))
                l1_val_reg_loss_per_epoch.append(np.mean(l1_val_reg_losses))
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    fig, ax = plt.subplots()
    ax.plot(l2_train_reg_loss_per_epoch, label = "train_reg_loss")
    ax.plot(l2_val_reg_loss_per_epoch, label = "val_reg_loss")
    ax.set_xlabel("epoch")
    ax.set_ylabel("loss")
    ax.set_title("L2 reg loss for func : " + args.time_change)
    leg = ax.legend()
    
    fig, ax = plt.subplots()
    ax.plot(l1_train_reg_loss_per_epoch, label = "train_reg_loss")
    ax.plot(l1_val_reg_loss_per_epoch, label = "val_reg_loss")
    ax.set_xlabel("epoch")
    ax.set_ylabel("loss")
    ax.set_title("L1 reg loss for func : " + args.time_change)
    leg = ax.legend()
    
    fig_name = "validation_loss_" + args.time_change + ".png"
    save_path = os.path.join(args.save, fig_name)
    fig, ax = plt.subplots()
    ax.plot(likli_val_loss_per_epoch)
    ax.set_title("Validation loss : " + args.time_change)
    plt.savefig(save_path, format = "png")
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_augmented_model_tabular
from ctfp_tools import run_ctfp_model as run_model, parse_arguments, plot_model
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_augmented_model_tabular(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change)#,
        #time_change_dims = args.time_change_dims
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    l2_val_reg_loss_per_epoch = []
    l1_val_reg_loss_per_epoch = []
    l2_train_reg_loss_per_epoch = []
    l1_train_reg_loss_per_epoch = []
    likli_val_loss_per_epoch = []
    num_best_epoch = 0
    start_time = time.time()
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        l2_train_reg_loss = []
        l1_train_reg_loss = []
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss, reg_loss_time_change = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
            if args.l2_reg_loss:
                loss = loss + reg_loss_time_change[0]
            if args.l1_reg_loss:
                loss = loss + reg_loss_time_change[1]
                
            l2_train_reg_loss.append(reg_loss_time_change[0].data.cpu().numpy())
            l1_train_reg_loss.append(reg_loss_time_change[1].data.cpu().numpy())
            
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                reg_loss_message = "Reg_loss l2: %.4f | Reg_loss l1: %.4f"
                print(reg_loss_message %(reg_loss_time_change[0].item(), reg_loss_time_change[1].item()))
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
        # mean l2 regularisation loss
        l2_train_reg_loss_per_epoch.append(np.mean(l2_train_reg_loss))
        l1_train_reg_loss_per_epoch.append(np.mean(l1_train_reg_loss))
        
        if epoch % plot_phi_freq == 0:
            
            time_change_func.show_plot_(times, epoch, args.time_change)
            plot_model(aug_model, times, epoch)
            
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                l2_val_reg_losses = []
                l1_val_reg_losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss, reg_loss_time_change = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                    
                    if args.l2_reg_loss:
                        loss = loss + reg_loss_time_change[0]
                    if args.l1_reg_loss:
                        loss = loss + reg_loss_time_change[1]
                        
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    l2_val_reg_losses.append(reg_loss_time_change[0].data.cpu().numpy())
                    l1_val_reg_losses.append(reg_loss_time_change[1].data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                likli_val_loss_per_epoch.append(loss)
                l2_val_reg_loss_per_epoch.append(np.mean(l2_val_reg_losses))
                l1_val_reg_loss_per_epoch.append(np.mean(l1_val_reg_losses))
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    fig, ax = plt.subplots()
    ax.plot(l2_train_reg_loss_per_epoch, label = "train_reg_loss")
    ax.plot(l2_val_reg_loss_per_epoch, label = "val_reg_loss")
    ax.set_xlabel("epoch")
    ax.set_ylabel("loss")
    ax.set_title("L2 reg loss for func : " + args.time_change)
    leg = ax.legend()
    
    fig, ax = plt.subplots()
    ax.plot(l1_train_reg_loss_per_epoch, label = "train_reg_loss")
    ax.plot(l1_val_reg_loss_per_epoch, label = "val_reg_loss")
    ax.set_xlabel("epoch")
    ax.set_ylabel("loss")
    ax.set_title("L1 reg loss for func : " + args.time_change)
    leg = ax.legend()
    
    fig_name = "validation_loss_" + args.time_change + ".png"
    save_path = os.path.join(args.save, fig_name)
    fig, ax = plt.subplots()
    ax.plot(likli_val_loss_per_epoch)
    ax.set_title("Validation loss : " + args.time_change)
    plt.savefig(save_path, format = "png")
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_augmented_model_tabular
from ctfp_tools import run_ctfp_model as run_model, parse_arguments, plot_model
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_augmented_model_tabular(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change)#,
        #time_change_dims = args.time_change_dims
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    l2_val_reg_loss_per_epoch = []
    l1_val_reg_loss_per_epoch = []
    l2_train_reg_loss_per_epoch = []
    l1_train_reg_loss_per_epoch = []
    likli_val_loss_per_epoch = []
    num_best_epoch = 0
    start_time = time.time()
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        l2_train_reg_loss = []
        l1_train_reg_loss = []
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss, reg_loss_time_change = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
            if args.l2_reg_loss:
                loss = loss + reg_loss_time_change[0]
            if args.l1_reg_loss:
                loss = loss + reg_loss_time_change[1]
                
            l2_train_reg_loss.append(reg_loss_time_change[0].data.cpu().numpy())
            l1_train_reg_loss.append(reg_loss_time_change[1].data.cpu().numpy())
            
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                reg_loss_message = "Reg_loss l2: %.4f | Reg_loss l1: %.4f"
                print(reg_loss_message %(reg_loss_time_change[0].item(), reg_loss_time_change[1].item()))
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
        # mean l2 regularisation loss
        l2_train_reg_loss_per_epoch.append(np.mean(l2_train_reg_loss))
        l1_train_reg_loss_per_epoch.append(np.mean(l1_train_reg_loss))
        
        if epoch % plot_phi_freq == 0:
            
            time_change_func.show_plot_(times, epoch, args.time_change)
            plot_model(aug_model, times, epoch)
            
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                l2_val_reg_losses = []
                l1_val_reg_losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss, reg_loss_time_change = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                    
                    if args.l2_reg_loss:
                        loss = loss + reg_loss_time_change[0]
                    if args.l1_reg_loss:
                        loss = loss + reg_loss_time_change[1]
                        
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    l2_val_reg_losses.append(reg_loss_time_change[0].data.cpu().numpy())
                    l1_val_reg_losses.append(reg_loss_time_change[1].data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                likli_val_loss_per_epoch.append(loss)
                l2_val_reg_loss_per_epoch.append(np.mean(l2_val_reg_losses))
                l1_val_reg_loss_per_epoch.append(np.mean(l1_val_reg_losses))
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    fig, ax = plt.subplots()
    ax.plot(l2_train_reg_loss_per_epoch, label = "train_reg_loss")
    ax.plot(l2_val_reg_loss_per_epoch, label = "val_reg_loss")
    ax.set_xlabel("epoch")
    ax.set_ylabel("loss")
    ax.set_title("L2 reg loss for func : " + args.time_change)
    leg = ax.legend()
    
    fig, ax = plt.subplots()
    ax.plot(l1_train_reg_loss_per_epoch, label = "train_reg_loss")
    ax.plot(l1_val_reg_loss_per_epoch, label = "val_reg_loss")
    ax.set_xlabel("epoch")
    ax.set_ylabel("loss")
    ax.set_title("L1 reg loss for func : " + args.time_change)
    leg = ax.legend()
    
    fig_name = "validation_loss_" + args.time_change + ".png"
    save_path = os.path.join(args.save, fig_name)
    fig, ax = plt.subplots()
    ax.plot(likli_val_loss_per_epoch)
    ax.set_title("Validation loss : " + args.time_change)
    plt.savefig(save_path, format = "png")
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change=None, time_change_dims=None, time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change=None, time_change_dims=None, time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change=None, time_change_dims=None, time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_augmented_model_tabular
from ctfp_tools import run_ctfp_model as run_model, parse_arguments, plot_model
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_augmented_model_tabular(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change)#,
        #time_change_dims = args.time_change_dims
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    l2_val_reg_loss_per_epoch = []
    l1_val_reg_loss_per_epoch = []
    l2_train_reg_loss_per_epoch = []
    l1_train_reg_loss_per_epoch = []
    likli_val_loss_per_epoch = []
    num_best_epoch = 0
    start_time = time.time()
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        l2_train_reg_loss = []
        l1_train_reg_loss = []
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss, reg_loss_time_change = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
            if args.l2_reg_loss:
                loss = loss + reg_loss_time_change[0]
            if args.l1_reg_loss:
                loss = loss + reg_loss_time_change[1]
                
            l2_train_reg_loss.append(reg_loss_time_change[0].data.cpu().numpy())
            l1_train_reg_loss.append(reg_loss_time_change[1].data.cpu().numpy())
            
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                reg_loss_message = "Reg_loss l2: %.4f | Reg_loss l1: %.4f"
                print(reg_loss_message %(reg_loss_time_change[0].item(), reg_loss_time_change[1].item()))
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
        # mean l2 regularisation loss
        l2_train_reg_loss_per_epoch.append(np.mean(l2_train_reg_loss))
        l1_train_reg_loss_per_epoch.append(np.mean(l1_train_reg_loss))
        
        if epoch % plot_phi_freq == 0:
            
            time_change_func.show_plot_(times, epoch, args.time_change)
            plot_model(aug_model, times, epoch)
            
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                l2_val_reg_losses = []
                l1_val_reg_losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss, reg_loss_time_change = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                    
                    if args.l2_reg_loss:
                        loss = loss + reg_loss_time_change[0]
                    if args.l1_reg_loss:
                        loss = loss + reg_loss_time_change[1]
                        
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    l2_val_reg_losses.append(reg_loss_time_change[0].data.cpu().numpy())
                    l1_val_reg_losses.append(reg_loss_time_change[1].data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                likli_val_loss_per_epoch.append(loss)
                l2_val_reg_loss_per_epoch.append(np.mean(l2_val_reg_losses))
                l1_val_reg_loss_per_epoch.append(np.mean(l1_val_reg_losses))
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    fig, ax = plt.subplots()
    ax.plot(l2_train_reg_loss_per_epoch, label = "train_reg_loss")
    ax.plot(l2_val_reg_loss_per_epoch, label = "val_reg_loss")
    ax.set_xlabel("epoch")
    ax.set_ylabel("loss")
    ax.set_title("L2 reg loss for func : " + args.time_change)
    leg = ax.legend()
    
    fig, ax = plt.subplots()
    ax.plot(l1_train_reg_loss_per_epoch, label = "train_reg_loss")
    ax.plot(l1_val_reg_loss_per_epoch, label = "val_reg_loss")
    ax.set_xlabel("epoch")
    ax.set_ylabel("loss")
    ax.set_title("L1 reg loss for func : " + args.time_change)
    leg = ax.legend()
    
    fig_name = "validation_loss_" + args.time_change + ".png"
    save_path = os.path.join(args.save, fig_name)
    fig, ax = plt.subplots()
    ax.plot(likli_val_loss_per_epoch)
    ax.set_title("Validation loss : " + args.time_change)
    plt.savefig(save_path, format = "png")
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_augmented_model_tabular
from ctfp_tools import run_ctfp_model as run_model, parse_arguments, plot_model
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_augmented_model_tabular(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change)#,
        #time_change_dims = args.time_change_dims
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    l2_val_reg_loss_per_epoch = []
    l1_val_reg_loss_per_epoch = []
    l2_train_reg_loss_per_epoch = []
    l1_train_reg_loss_per_epoch = []
    likli_val_loss_per_epoch = []
    num_best_epoch = 0
    start_time = time.time()
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        l2_train_reg_loss = []
        l1_train_reg_loss = []
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss, reg_loss_time_change = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
            if args.l2_reg_loss:
                loss = loss + reg_loss_time_change[0]
            if args.l1_reg_loss:
                loss = loss + reg_loss_time_change[1]
                
            l2_train_reg_loss.append(reg_loss_time_change[0].data.cpu().numpy())
            l1_train_reg_loss.append(reg_loss_time_change[1].data.cpu().numpy())
            
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                reg_loss_message = "Reg_loss l2: %.4f | Reg_loss l1: %.4f"
                print(reg_loss_message %(reg_loss_time_change[0].item(), reg_loss_time_change[1].item()))
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
        # mean l2 regularisation loss
        l2_train_reg_loss_per_epoch.append(np.mean(l2_train_reg_loss))
        l1_train_reg_loss_per_epoch.append(np.mean(l1_train_reg_loss))
        
        if epoch % plot_phi_freq == 0:
            
            time_change_func.show_plot_(times, epoch, args.time_change)
            plot_model(aug_model, times, epoch)
            
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                l2_val_reg_losses = []
                l1_val_reg_losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss, reg_loss_time_change = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                    
                    if args.l2_reg_loss:
                        loss = loss + reg_loss_time_change[0]
                    if args.l1_reg_loss:
                        loss = loss + reg_loss_time_change[1]
                        
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    l2_val_reg_losses.append(reg_loss_time_change[0].data.cpu().numpy())
                    l1_val_reg_losses.append(reg_loss_time_change[1].data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                likli_val_loss_per_epoch.append(loss)
                l2_val_reg_loss_per_epoch.append(np.mean(l2_val_reg_losses))
                l1_val_reg_loss_per_epoch.append(np.mean(l1_val_reg_losses))
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    fig, ax = plt.subplots()
    ax.plot(l2_train_reg_loss_per_epoch, label = "train_reg_loss")
    ax.plot(l2_val_reg_loss_per_epoch, label = "val_reg_loss")
    ax.set_xlabel("epoch")
    ax.set_ylabel("loss")
    ax.set_title("L2 reg loss for func : " + args.time_change)
    leg = ax.legend()
    
    fig, ax = plt.subplots()
    ax.plot(l1_train_reg_loss_per_epoch, label = "train_reg_loss")
    ax.plot(l1_val_reg_loss_per_epoch, label = "val_reg_loss")
    ax.set_xlabel("epoch")
    ax.set_ylabel("loss")
    ax.set_title("L1 reg loss for func : " + args.time_change)
    leg = ax.legend()
    
    fig_name = "validation_loss_" + args.time_change + ".png"
    save_path = os.path.join(args.save, fig_name)
    fig, ax = plt.subplots()
    ax.plot(likli_val_loss_per_epoch)
    ax.set_title("Validation loss : " + args.time_change)
    plt.savefig(save_path, format = "png")
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_augmented_model_tabular
from ctfp_tools import run_ctfp_model as run_model, parse_arguments, plot_model
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_augmented_model_tabular(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change)#,
        #time_change_dims = args.time_change_dims
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    l2_val_reg_loss_per_epoch = []
    l1_val_reg_loss_per_epoch = []
    l2_train_reg_loss_per_epoch = []
    l1_train_reg_loss_per_epoch = []
    likli_val_loss_per_epoch = []
    num_best_epoch = 0
    start_time = time.time()
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        l2_train_reg_loss = []
        l1_train_reg_loss = []
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss, reg_loss_time_change = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
            if args.l2_reg_loss:
                loss = loss + reg_loss_time_change[0]
            if args.l1_reg_loss:
                loss = loss + reg_loss_time_change[1]
                
            l2_train_reg_loss.append(reg_loss_time_change[0].data.cpu().numpy())
            l1_train_reg_loss.append(reg_loss_time_change[1].data.cpu().numpy())
            
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                reg_loss_message = "Reg_loss l2: %.4f | Reg_loss l1: %.4f"
                print(reg_loss_message %(reg_loss_time_change[0].item(), reg_loss_time_change[1].item()))
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
        # mean l2 regularisation loss
        l2_train_reg_loss_per_epoch.append(np.mean(l2_train_reg_loss))
        l1_train_reg_loss_per_epoch.append(np.mean(l1_train_reg_loss))
        
        if epoch % plot_phi_freq == 0:
            
            time_change_func.show_plot_(times, epoch, args.time_change)
            plot_model(aug_model, times, epoch)
            
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                l2_val_reg_losses = []
                l1_val_reg_losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss, reg_loss_time_change = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                    
                    if args.l2_reg_loss:
                        loss = loss + reg_loss_time_change[0]
                    if args.l1_reg_loss:
                        loss = loss + reg_loss_time_change[1]
                        
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    l2_val_reg_losses.append(reg_loss_time_change[0].data.cpu().numpy())
                    l1_val_reg_losses.append(reg_loss_time_change[1].data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                likli_val_loss_per_epoch.append(loss)
                l2_val_reg_loss_per_epoch.append(np.mean(l2_val_reg_losses))
                l1_val_reg_loss_per_epoch.append(np.mean(l1_val_reg_losses))
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    fig, ax = plt.subplots()
    ax.plot(l2_train_reg_loss_per_epoch, label = "train_reg_loss")
    ax.plot(l2_val_reg_loss_per_epoch, label = "val_reg_loss")
    ax.set_xlabel("epoch")
    ax.set_ylabel("loss")
    ax.set_title("L2 reg loss for func : " + args.time_change)
    leg = ax.legend()
    
    fig, ax = plt.subplots()
    ax.plot(l1_train_reg_loss_per_epoch, label = "train_reg_loss")
    ax.plot(l1_val_reg_loss_per_epoch, label = "val_reg_loss")
    ax.set_xlabel("epoch")
    ax.set_ylabel("loss")
    ax.set_title("L1 reg loss for func : " + args.time_change)
    leg = ax.legend()
    
    fig_name = "validation_loss_" + args.time_change + ".png"
    save_path = os.path.join(args.save, fig_name)
    fig, ax = plt.subplots()
    ax.plot(likli_val_loss_per_epoch)
    ax.set_title("Validation loss : " + args.time_change)
    plt.savefig(save_path, format = "png")
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_augmented_model_tabular
from ctfp_tools import run_ctfp_model as run_model, parse_arguments, plot_model
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_augmented_model_tabular(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change)#,
        #time_change_dims = args.time_change_dims
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    l2_val_reg_loss_per_epoch = []
    l1_val_reg_loss_per_epoch = []
    l2_train_reg_loss_per_epoch = []
    l1_train_reg_loss_per_epoch = []
    likli_val_loss_per_epoch = []
    num_best_epoch = 0
    start_time = time.time()
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        l2_train_reg_loss = []
        l1_train_reg_loss = []
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss, reg_loss_time_change = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
            if args.l2_reg_loss:
                loss = loss + reg_loss_time_change[0]
            if args.l1_reg_loss:
                loss = loss + reg_loss_time_change[1]
                
            l2_train_reg_loss.append(reg_loss_time_change[0].data.cpu().numpy())
            l1_train_reg_loss.append(reg_loss_time_change[1].data.cpu().numpy())
            
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                reg_loss_message = "Reg_loss l2: %.4f | Reg_loss l1: %.4f"
                print(reg_loss_message %(reg_loss_time_change[0].item(), reg_loss_time_change[1].item()))
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
        # mean l2 regularisation loss
        l2_train_reg_loss_per_epoch.append(np.mean(l2_train_reg_loss))
        l1_train_reg_loss_per_epoch.append(np.mean(l1_train_reg_loss))
        
        if epoch % plot_phi_freq == 0:
            
            time_change_func.show_plot_(times, epoch, args.time_change)
            plot_model(aug_model, times, epoch)
            
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                l2_val_reg_losses = []
                l1_val_reg_losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss, reg_loss_time_change = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                    
                    if args.l2_reg_loss:
                        loss = loss + reg_loss_time_change[0]
                    if args.l1_reg_loss:
                        loss = loss + reg_loss_time_change[1]
                        
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    l2_val_reg_losses.append(reg_loss_time_change[0].data.cpu().numpy())
                    l1_val_reg_losses.append(reg_loss_time_change[1].data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                likli_val_loss_per_epoch.append(loss)
                l2_val_reg_loss_per_epoch.append(np.mean(l2_val_reg_losses))
                l1_val_reg_loss_per_epoch.append(np.mean(l1_val_reg_losses))
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    fig, ax = plt.subplots()
    ax.plot(l2_train_reg_loss_per_epoch, label = "train_reg_loss")
    ax.plot(l2_val_reg_loss_per_epoch, label = "val_reg_loss")
    ax.set_xlabel("epoch")
    ax.set_ylabel("loss")
    ax.set_title("L2 reg loss for func : " + args.time_change)
    leg = ax.legend()
    
    fig, ax = plt.subplots()
    ax.plot(l1_train_reg_loss_per_epoch, label = "train_reg_loss")
    ax.plot(l1_val_reg_loss_per_epoch, label = "val_reg_loss")
    ax.set_xlabel("epoch")
    ax.set_ylabel("loss")
    ax.set_title("L1 reg loss for func : " + args.time_change)
    leg = ax.legend()
    
    fig_name = "validation_loss_" + args.time_change + ".png"
    save_path = os.path.join(args.save, fig_name)
    fig, ax = plt.subplots()
    ax.plot(likli_val_loss_per_epoch)
    ax.set_title("Validation loss : " + args.time_change)
    plt.savefig(save_path, format = "png")
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change='id', time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change='id', time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change='id', time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change='id', time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
TimeChangedCNF(
  (sequential_flow): SequentialFlow(
    (chain): ModuleList(
      (0): CNF(
        (odefunc): RegularizedODEfunc(
          (odefunc): AugODEfunc(
            (diffeq): AugODEnet(
              (layers): ModuleList(
                (0): IgnoreLinear(
                  (_layer): Linear(in_features=2, out_features=8, bias=True)
                )
                (1): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=32, bias=True)
                )
                (2): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=32, bias=True)
                )
                (3): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=8, bias=True)
                )
                (4): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=1, bias=True)
                )
              )
              (activation_fns): ModuleList(
                (0): Softplus(beta=1, threshold=20)
                (1): Softplus(beta=1, threshold=20)
                (2): Softplus(beta=1, threshold=20)
                (3): Softplus(beta=1, threshold=20)
              )
            )
          )
        )
      )
    )
  )
  (time_change_func): time_change_func_id()
)
TimeChangedCNF(
  (sequential_flow): SequentialFlow(
    (chain): ModuleList(
      (0): CNF(
        (odefunc): RegularizedODEfunc(
          (odefunc): AugODEfunc(
            (diffeq): AugODEnet(
              (layers): ModuleList(
                (0): IgnoreLinear(
                  (_layer): Linear(in_features=2, out_features=8, bias=True)
                )
                (1): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=32, bias=True)
                )
                (2): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=32, bias=True)
                )
                (3): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=8, bias=True)
                )
                (4): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=1, bias=True)
                )
              )
              (activation_fns): ModuleList(
                (0): Softplus(beta=1, threshold=20)
                (1): Softplus(beta=1, threshold=20)
                (2): Softplus(beta=1, threshold=20)
                (3): Softplus(beta=1, threshold=20)
              )
            )
          )
        )
      )
    )
  )
  (time_change_func): time_change_func_id()
)
TimeChangedCNF(
  (sequential_flow): SequentialFlow(
    (chain): ModuleList(
      (0): CNF(
        (odefunc): RegularizedODEfunc(
          (odefunc): AugODEfunc(
            (diffeq): AugODEnet(
              (layers): ModuleList(
                (0): IgnoreLinear(
                  (_layer): Linear(in_features=2, out_features=8, bias=True)
                )
                (1): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=32, bias=True)
                )
                (2): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=32, bias=True)
                )
                (3): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=8, bias=True)
                )
                (4): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=1, bias=True)
                )
              )
              (activation_fns): ModuleList(
                (0): Softplus(beta=1, threshold=20)
                (1): Softplus(beta=1, threshold=20)
                (2): Softplus(beta=1, threshold=20)
                (3): Softplus(beta=1, threshold=20)
              )
            )
          )
        )
      )
    )
  )
  (time_change_func): time_change_func_id()
)
TimeChangedCNF(
  (sequential_flow): SequentialFlow(
    (chain): ModuleList(
      (0): CNF(
        (odefunc): RegularizedODEfunc(
          (odefunc): AugODEfunc(
            (diffeq): AugODEnet(
              (layers): ModuleList(
                (0): IgnoreLinear(
                  (_layer): Linear(in_features=2, out_features=8, bias=True)
                )
                (1): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=32, bias=True)
                )
                (2): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=32, bias=True)
                )
                (3): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=8, bias=True)
                )
                (4): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=1, bias=True)
                )
              )
              (activation_fns): ModuleList(
                (0): Softplus(beta=1, threshold=20)
                (1): Softplus(beta=1, threshold=20)
                (2): Softplus(beta=1, threshold=20)
                (3): Softplus(beta=1, threshold=20)
              )
            )
          )
        )
      )
    )
  )
  (time_change_func): time_change_func_id()
)
Number of trainable parameters: 1642
Number of trainable parameters: 1642
Number of trainable parameters: 1642
Number of trainable parameters: 1642
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_augmented_model_tabular
from ctfp_tools import run_tcnf_model as run_model, parse_arguments
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_augmented_model_tabular(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change
    )
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    start_time = time.time()
    
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
                        
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
                    
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                                            
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_augmented_model_tabular
from ctfp_tools import run_tcnf_model as run_model, parse_arguments
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_augmented_model_tabular(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change
    )
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    start_time = time.time()
    
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
                        
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
                    
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                                            
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_augmented_model_tabular
from ctfp_tools import run_tcnf_model as run_model, parse_arguments
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_augmented_model_tabular(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change
    )
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    start_time = time.time()
    
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
                        
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
                    
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                                            
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_augmented_model_tabular
from ctfp_tools import run_tcnf_model as run_model, parse_arguments
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_augmented_model_tabular(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change
    )
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    start_time = time.time()
    
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
                        
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
                    
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                                            
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_augmented_model_tabular
from ctfp_tools import run_tcnf_model as run_model, parse_arguments
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_augmented_model_tabular(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change
    )
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    start_time = time.time()
    
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
                        
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
                    
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                                            
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change='id', time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change='id', time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change='id', time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change='id', time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change='id', time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
TimeChangedCNF(
  (sequential_flow): SequentialFlow(
    (chain): ModuleList(
      (0): CNF(
        (odefunc): RegularizedODEfunc(
          (odefunc): AugODEfunc(
            (diffeq): AugODEnet(
              (layers): ModuleList(
                (0): IgnoreLinear(
                  (_layer): Linear(in_features=2, out_features=8, bias=True)
                )
                (1): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=32, bias=True)
                )
                (2): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=32, bias=True)
                )
                (3): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=8, bias=True)
                )
                (4): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=1, bias=True)
                )
              )
              (activation_fns): ModuleList(
                (0): Softplus(beta=1, threshold=20)
                (1): Softplus(beta=1, threshold=20)
                (2): Softplus(beta=1, threshold=20)
                (3): Softplus(beta=1, threshold=20)
              )
            )
          )
        )
      )
    )
  )
  (time_change_func): time_change_func_id()
)
TimeChangedCNF(
  (sequential_flow): SequentialFlow(
    (chain): ModuleList(
      (0): CNF(
        (odefunc): RegularizedODEfunc(
          (odefunc): AugODEfunc(
            (diffeq): AugODEnet(
              (layers): ModuleList(
                (0): IgnoreLinear(
                  (_layer): Linear(in_features=2, out_features=8, bias=True)
                )
                (1): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=32, bias=True)
                )
                (2): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=32, bias=True)
                )
                (3): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=8, bias=True)
                )
                (4): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=1, bias=True)
                )
              )
              (activation_fns): ModuleList(
                (0): Softplus(beta=1, threshold=20)
                (1): Softplus(beta=1, threshold=20)
                (2): Softplus(beta=1, threshold=20)
                (3): Softplus(beta=1, threshold=20)
              )
            )
          )
        )
      )
    )
  )
  (time_change_func): time_change_func_id()
)
TimeChangedCNF(
  (sequential_flow): SequentialFlow(
    (chain): ModuleList(
      (0): CNF(
        (odefunc): RegularizedODEfunc(
          (odefunc): AugODEfunc(
            (diffeq): AugODEnet(
              (layers): ModuleList(
                (0): IgnoreLinear(
                  (_layer): Linear(in_features=2, out_features=8, bias=True)
                )
                (1): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=32, bias=True)
                )
                (2): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=32, bias=True)
                )
                (3): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=8, bias=True)
                )
                (4): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=1, bias=True)
                )
              )
              (activation_fns): ModuleList(
                (0): Softplus(beta=1, threshold=20)
                (1): Softplus(beta=1, threshold=20)
                (2): Softplus(beta=1, threshold=20)
                (3): Softplus(beta=1, threshold=20)
              )
            )
          )
        )
      )
    )
  )
  (time_change_func): time_change_func_id()
)
TimeChangedCNF(
  (sequential_flow): SequentialFlow(
    (chain): ModuleList(
      (0): CNF(
        (odefunc): RegularizedODEfunc(
          (odefunc): AugODEfunc(
            (diffeq): AugODEnet(
              (layers): ModuleList(
                (0): IgnoreLinear(
                  (_layer): Linear(in_features=2, out_features=8, bias=True)
                )
                (1): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=32, bias=True)
                )
                (2): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=32, bias=True)
                )
                (3): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=8, bias=True)
                )
                (4): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=1, bias=True)
                )
              )
              (activation_fns): ModuleList(
                (0): Softplus(beta=1, threshold=20)
                (1): Softplus(beta=1, threshold=20)
                (2): Softplus(beta=1, threshold=20)
                (3): Softplus(beta=1, threshold=20)
              )
            )
          )
        )
      )
    )
  )
  (time_change_func): time_change_func_id()
)
TimeChangedCNF(
  (sequential_flow): SequentialFlow(
    (chain): ModuleList(
      (0): CNF(
        (odefunc): RegularizedODEfunc(
          (odefunc): AugODEfunc(
            (diffeq): AugODEnet(
              (layers): ModuleList(
                (0): IgnoreLinear(
                  (_layer): Linear(in_features=2, out_features=8, bias=True)
                )
                (1): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=32, bias=True)
                )
                (2): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=32, bias=True)
                )
                (3): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=8, bias=True)
                )
                (4): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=1, bias=True)
                )
              )
              (activation_fns): ModuleList(
                (0): Softplus(beta=1, threshold=20)
                (1): Softplus(beta=1, threshold=20)
                (2): Softplus(beta=1, threshold=20)
                (3): Softplus(beta=1, threshold=20)
              )
            )
          )
        )
      )
    )
  )
  (time_change_func): time_change_func_id()
)
Number of trainable parameters: 1642
Number of trainable parameters: 1642
Number of trainable parameters: 1642
Number of trainable parameters: 1642
Number of trainable parameters: 1642
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_augmented_model_tabular
from ctfp_tools import run_tcnf_model as run_model, parse_arguments
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_augmented_model_tabular(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change
    )
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    start_time = time.time()
    
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
                        
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
                    
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                                            
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_augmented_model_tabular
from ctfp_tools import run_tcnf_model as run_model, parse_arguments
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_augmented_model_tabular(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change
    )
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    start_time = time.time()
    
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
                        
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
                    
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                                            
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_augmented_model_tabular
from ctfp_tools import run_tcnf_model as run_model, parse_arguments
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_augmented_model_tabular(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change
    )
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    start_time = time.time()
    
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
                        
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
                    
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                                            
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_augmented_model_tabular
from ctfp_tools import run_tcnf_model as run_model, parse_arguments
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_augmented_model_tabular(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change
    )
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    start_time = time.time()
    
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
                        
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
                    
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                                            
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_augmented_model_tabular
from ctfp_tools import run_tcnf_model as run_model, parse_arguments
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_augmented_model_tabular(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change
    )
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    start_time = time.time()
    
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
                        
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
                    
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                                            
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_augmented_model_tabular
from ctfp_tools import run_tcnf_model as run_model, parse_arguments
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_augmented_model_tabular(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change
    )
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    start_time = time.time()
    
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
                        
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
                    
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                                            
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change='id', time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change='id', time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change='id', time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change='id', time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change='id', time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change='id', time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
TimeChangedCNF(
  (sequential_flow): SequentialFlow(
    (chain): ModuleList(
      (0): CNF(
        (odefunc): RegularizedODEfunc(
          (odefunc): AugODEfunc(
            (diffeq): AugODEnet(
              (layers): ModuleList(
                (0): IgnoreLinear(
                  (_layer): Linear(in_features=2, out_features=8, bias=True)
                )
                (1): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=32, bias=True)
                )
                (2): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=32, bias=True)
                )
                (3): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=8, bias=True)
                )
                (4): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=1, bias=True)
                )
              )
              (activation_fns): ModuleList(
                (0): Softplus(beta=1, threshold=20)
                (1): Softplus(beta=1, threshold=20)
                (2): Softplus(beta=1, threshold=20)
                (3): Softplus(beta=1, threshold=20)
              )
            )
          )
        )
      )
    )
  )
  (time_change_func): time_change_func_id()
)
TimeChangedCNF(
  (sequential_flow): SequentialFlow(
    (chain): ModuleList(
      (0): CNF(
        (odefunc): RegularizedODEfunc(
          (odefunc): AugODEfunc(
            (diffeq): AugODEnet(
              (layers): ModuleList(
                (0): IgnoreLinear(
                  (_layer): Linear(in_features=2, out_features=8, bias=True)
                )
                (1): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=32, bias=True)
                )
                (2): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=32, bias=True)
                )
                (3): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=8, bias=True)
                )
                (4): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=1, bias=True)
                )
              )
              (activation_fns): ModuleList(
                (0): Softplus(beta=1, threshold=20)
                (1): Softplus(beta=1, threshold=20)
                (2): Softplus(beta=1, threshold=20)
                (3): Softplus(beta=1, threshold=20)
              )
            )
          )
        )
      )
    )
  )
  (time_change_func): time_change_func_id()
)
TimeChangedCNF(
  (sequential_flow): SequentialFlow(
    (chain): ModuleList(
      (0): CNF(
        (odefunc): RegularizedODEfunc(
          (odefunc): AugODEfunc(
            (diffeq): AugODEnet(
              (layers): ModuleList(
                (0): IgnoreLinear(
                  (_layer): Linear(in_features=2, out_features=8, bias=True)
                )
                (1): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=32, bias=True)
                )
                (2): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=32, bias=True)
                )
                (3): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=8, bias=True)
                )
                (4): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=1, bias=True)
                )
              )
              (activation_fns): ModuleList(
                (0): Softplus(beta=1, threshold=20)
                (1): Softplus(beta=1, threshold=20)
                (2): Softplus(beta=1, threshold=20)
                (3): Softplus(beta=1, threshold=20)
              )
            )
          )
        )
      )
    )
  )
  (time_change_func): time_change_func_id()
)
TimeChangedCNF(
  (sequential_flow): SequentialFlow(
    (chain): ModuleList(
      (0): CNF(
        (odefunc): RegularizedODEfunc(
          (odefunc): AugODEfunc(
            (diffeq): AugODEnet(
              (layers): ModuleList(
                (0): IgnoreLinear(
                  (_layer): Linear(in_features=2, out_features=8, bias=True)
                )
                (1): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=32, bias=True)
                )
                (2): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=32, bias=True)
                )
                (3): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=8, bias=True)
                )
                (4): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=1, bias=True)
                )
              )
              (activation_fns): ModuleList(
                (0): Softplus(beta=1, threshold=20)
                (1): Softplus(beta=1, threshold=20)
                (2): Softplus(beta=1, threshold=20)
                (3): Softplus(beta=1, threshold=20)
              )
            )
          )
        )
      )
    )
  )
  (time_change_func): time_change_func_id()
)
TimeChangedCNF(
  (sequential_flow): SequentialFlow(
    (chain): ModuleList(
      (0): CNF(
        (odefunc): RegularizedODEfunc(
          (odefunc): AugODEfunc(
            (diffeq): AugODEnet(
              (layers): ModuleList(
                (0): IgnoreLinear(
                  (_layer): Linear(in_features=2, out_features=8, bias=True)
                )
                (1): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=32, bias=True)
                )
                (2): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=32, bias=True)
                )
                (3): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=8, bias=True)
                )
                (4): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=1, bias=True)
                )
              )
              (activation_fns): ModuleList(
                (0): Softplus(beta=1, threshold=20)
                (1): Softplus(beta=1, threshold=20)
                (2): Softplus(beta=1, threshold=20)
                (3): Softplus(beta=1, threshold=20)
              )
            )
          )
        )
      )
    )
  )
  (time_change_func): time_change_func_id()
)
TimeChangedCNF(
  (sequential_flow): SequentialFlow(
    (chain): ModuleList(
      (0): CNF(
        (odefunc): RegularizedODEfunc(
          (odefunc): AugODEfunc(
            (diffeq): AugODEnet(
              (layers): ModuleList(
                (0): IgnoreLinear(
                  (_layer): Linear(in_features=2, out_features=8, bias=True)
                )
                (1): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=32, bias=True)
                )
                (2): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=32, bias=True)
                )
                (3): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=8, bias=True)
                )
                (4): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=1, bias=True)
                )
              )
              (activation_fns): ModuleList(
                (0): Softplus(beta=1, threshold=20)
                (1): Softplus(beta=1, threshold=20)
                (2): Softplus(beta=1, threshold=20)
                (3): Softplus(beta=1, threshold=20)
              )
            )
          )
        )
      )
    )
  )
  (time_change_func): time_change_func_id()
)
Number of trainable parameters: 1642
Number of trainable parameters: 1642
Number of trainable parameters: 1642
Number of trainable parameters: 1642
Number of trainable parameters: 1642
Number of trainable parameters: 1642
Iter 0000 | Time 97.8347(97.8347) | Bit/dim -0.9779(-0.9779) | Steps 0(0.00) | Grad Norm 0.0352(0.0352) | Total Time 1.00(1.00)
Iter 0000 | Time 97.8347(97.8347) | Bit/dim -0.9779(-0.9779) | Steps 0(0.00) | Grad Norm 0.0352(0.0352) | Total Time 1.00(1.00)
Iter 0000 | Time 97.8347(97.8347) | Bit/dim -0.9779(-0.9779) | Steps 0(0.00) | Grad Norm 0.0352(0.0352) | Total Time 1.00(1.00)
Iter 0000 | Time 97.8347(97.8347) | Bit/dim -0.9779(-0.9779) | Steps 0(0.00) | Grad Norm 0.0352(0.0352) | Total Time 1.00(1.00)
Iter 0000 | Time 97.8347(97.8347) | Bit/dim -0.9779(-0.9779) | Steps 0(0.00) | Grad Norm 0.0352(0.0352) | Total Time 1.00(1.00)
Iter 0000 | Time 97.8347(97.8347) | Bit/dim -0.9779(-0.9779) | Steps 0(0.00) | Grad Norm 0.0352(0.0352) | Total Time 1.00(1.00)
Iter 0010 | Time 0.2140(2.9704) | Bit/dim -0.9579(-0.9325) | Steps 0(0.00) | Grad Norm 0.0253(0.0250) | Total Time 1.00(1.00)
Iter 0010 | Time 0.2140(2.9704) | Bit/dim -0.9579(-0.9325) | Steps 0(0.00) | Grad Norm 0.0253(0.0250) | Total Time 1.00(1.00)
Iter 0010 | Time 0.2140(2.9704) | Bit/dim -0.9579(-0.9325) | Steps 0(0.00) | Grad Norm 0.0253(0.0250) | Total Time 1.00(1.00)
Iter 0010 | Time 0.2140(2.9704) | Bit/dim -0.9579(-0.9325) | Steps 0(0.00) | Grad Norm 0.0253(0.0250) | Total Time 1.00(1.00)
Iter 0010 | Time 0.2140(2.9704) | Bit/dim -0.9579(-0.9325) | Steps 0(0.00) | Grad Norm 0.0253(0.0250) | Total Time 1.00(1.00)
Iter 0010 | Time 0.2140(2.9704) | Bit/dim -0.9579(-0.9325) | Steps 0(0.00) | Grad Norm 0.0253(0.0250) | Total Time 1.00(1.00)
Iter 0020 | Time 0.2139(0.2934) | Bit/dim -0.9334(-0.9278) | Steps 0(0.00) | Grad Norm 0.0387(0.0356) | Total Time 1.00(1.00)
Iter 0020 | Time 0.2139(0.2934) | Bit/dim -0.9334(-0.9278) | Steps 0(0.00) | Grad Norm 0.0387(0.0356) | Total Time 1.00(1.00)
Iter 0020 | Time 0.2139(0.2934) | Bit/dim -0.9334(-0.9278) | Steps 0(0.00) | Grad Norm 0.0387(0.0356) | Total Time 1.00(1.00)
Iter 0020 | Time 0.2139(0.2934) | Bit/dim -0.9334(-0.9278) | Steps 0(0.00) | Grad Norm 0.0387(0.0356) | Total Time 1.00(1.00)
Iter 0020 | Time 0.2139(0.2934) | Bit/dim -0.9334(-0.9278) | Steps 0(0.00) | Grad Norm 0.0387(0.0356) | Total Time 1.00(1.00)
Iter 0020 | Time 0.2139(0.2934) | Bit/dim -0.9334(-0.9278) | Steps 0(0.00) | Grad Norm 0.0387(0.0356) | Total Time 1.00(1.00)
Iter 0030 | Time 0.2560(0.2301) | Bit/dim -0.9494(-0.9611) | Steps 0(0.00) | Grad Norm 0.0567(0.0528) | Total Time 1.00(1.00)
Iter 0030 | Time 0.2560(0.2301) | Bit/dim -0.9494(-0.9611) | Steps 0(0.00) | Grad Norm 0.0567(0.0528) | Total Time 1.00(1.00)
Iter 0030 | Time 0.2560(0.2301) | Bit/dim -0.9494(-0.9611) | Steps 0(0.00) | Grad Norm 0.0567(0.0528) | Total Time 1.00(1.00)
Iter 0030 | Time 0.2560(0.2301) | Bit/dim -0.9494(-0.9611) | Steps 0(0.00) | Grad Norm 0.0567(0.0528) | Total Time 1.00(1.00)
Iter 0030 | Time 0.2560(0.2301) | Bit/dim -0.9494(-0.9611) | Steps 0(0.00) | Grad Norm 0.0567(0.0528) | Total Time 1.00(1.00)
Iter 0030 | Time 0.2560(0.2301) | Bit/dim -0.9494(-0.9611) | Steps 0(0.00) | Grad Norm 0.0567(0.0528) | Total Time 1.00(1.00)
validating...
validating...
validating...
validating...
validating...
validating...
Epoch 0001 | Time 6.8263, Bit/dim -0.9518
Epoch 0001 | Time 6.8263, Bit/dim -0.9518
Epoch 0001 | Time 6.8263, Bit/dim -0.9518
Epoch 0001 | Time 6.8263, Bit/dim -0.9518
Epoch 0001 | Time 6.8263, Bit/dim -0.9518
Epoch 0001 | Time 6.8263, Bit/dim -0.9518
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_TCNF
from ctfp_tools import run_tcnf_model as run_model, parse_arguments
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_TCNF(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change
    )
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    start_time = time.time()
    
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
                        
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
                    
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                                            
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_TCNF
from ctfp_tools import run_tcnf_model as run_model, parse_arguments
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_TCNF(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change
    )
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    start_time = time.time()
    
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
                        
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
                    
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                                            
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_TCNF
from ctfp_tools import run_tcnf_model as run_model, parse_arguments
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_TCNF(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change
    )
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    start_time = time.time()
    
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
                        
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
                    
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                                            
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_TCNF
from ctfp_tools import run_tcnf_model as run_model, parse_arguments
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_TCNF(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change
    )
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    start_time = time.time()
    
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
                        
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
                    
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                                            
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_TCNF
from ctfp_tools import run_tcnf_model as run_model, parse_arguments
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_TCNF(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change
    )
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    start_time = time.time()
    
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
                        
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
                    
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                                            
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_TCNF
from ctfp_tools import run_tcnf_model as run_model, parse_arguments
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_TCNF(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change
    )
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    start_time = time.time()
    
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
                        
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
                    
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                                            
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from ctfp_tools import build_TCNF
from ctfp_tools import run_tcnf_model as run_model, parse_arguments
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_TCNF(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change
    )
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    start_time = time.time()
    
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
                        
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
                    
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                                            
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change='id', time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change='id', time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change='id', time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change='id', time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change='id', time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change='id', time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change='id', time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
TimeChangedCNF(
  (sequential_flow): SequentialFlow(
    (chain): ModuleList(
      (0): CNF(
        (odefunc): RegularizedODEfunc(
          (odefunc): AugODEfunc(
            (diffeq): AugODEnet(
              (layers): ModuleList(
                (0): IgnoreLinear(
                  (_layer): Linear(in_features=2, out_features=8, bias=True)
                )
                (1): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=32, bias=True)
                )
                (2): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=32, bias=True)
                )
                (3): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=8, bias=True)
                )
                (4): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=1, bias=True)
                )
              )
              (activation_fns): ModuleList(
                (0): Softplus(beta=1, threshold=20)
                (1): Softplus(beta=1, threshold=20)
                (2): Softplus(beta=1, threshold=20)
                (3): Softplus(beta=1, threshold=20)
              )
            )
          )
        )
      )
    )
  )
  (time_change_func): time_change_func_id()
)
TimeChangedCNF(
  (sequential_flow): SequentialFlow(
    (chain): ModuleList(
      (0): CNF(
        (odefunc): RegularizedODEfunc(
          (odefunc): AugODEfunc(
            (diffeq): AugODEnet(
              (layers): ModuleList(
                (0): IgnoreLinear(
                  (_layer): Linear(in_features=2, out_features=8, bias=True)
                )
                (1): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=32, bias=True)
                )
                (2): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=32, bias=True)
                )
                (3): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=8, bias=True)
                )
                (4): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=1, bias=True)
                )
              )
              (activation_fns): ModuleList(
                (0): Softplus(beta=1, threshold=20)
                (1): Softplus(beta=1, threshold=20)
                (2): Softplus(beta=1, threshold=20)
                (3): Softplus(beta=1, threshold=20)
              )
            )
          )
        )
      )
    )
  )
  (time_change_func): time_change_func_id()
)
TimeChangedCNF(
  (sequential_flow): SequentialFlow(
    (chain): ModuleList(
      (0): CNF(
        (odefunc): RegularizedODEfunc(
          (odefunc): AugODEfunc(
            (diffeq): AugODEnet(
              (layers): ModuleList(
                (0): IgnoreLinear(
                  (_layer): Linear(in_features=2, out_features=8, bias=True)
                )
                (1): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=32, bias=True)
                )
                (2): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=32, bias=True)
                )
                (3): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=8, bias=True)
                )
                (4): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=1, bias=True)
                )
              )
              (activation_fns): ModuleList(
                (0): Softplus(beta=1, threshold=20)
                (1): Softplus(beta=1, threshold=20)
                (2): Softplus(beta=1, threshold=20)
                (3): Softplus(beta=1, threshold=20)
              )
            )
          )
        )
      )
    )
  )
  (time_change_func): time_change_func_id()
)
TimeChangedCNF(
  (sequential_flow): SequentialFlow(
    (chain): ModuleList(
      (0): CNF(
        (odefunc): RegularizedODEfunc(
          (odefunc): AugODEfunc(
            (diffeq): AugODEnet(
              (layers): ModuleList(
                (0): IgnoreLinear(
                  (_layer): Linear(in_features=2, out_features=8, bias=True)
                )
                (1): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=32, bias=True)
                )
                (2): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=32, bias=True)
                )
                (3): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=8, bias=True)
                )
                (4): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=1, bias=True)
                )
              )
              (activation_fns): ModuleList(
                (0): Softplus(beta=1, threshold=20)
                (1): Softplus(beta=1, threshold=20)
                (2): Softplus(beta=1, threshold=20)
                (3): Softplus(beta=1, threshold=20)
              )
            )
          )
        )
      )
    )
  )
  (time_change_func): time_change_func_id()
)
TimeChangedCNF(
  (sequential_flow): SequentialFlow(
    (chain): ModuleList(
      (0): CNF(
        (odefunc): RegularizedODEfunc(
          (odefunc): AugODEfunc(
            (diffeq): AugODEnet(
              (layers): ModuleList(
                (0): IgnoreLinear(
                  (_layer): Linear(in_features=2, out_features=8, bias=True)
                )
                (1): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=32, bias=True)
                )
                (2): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=32, bias=True)
                )
                (3): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=8, bias=True)
                )
                (4): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=1, bias=True)
                )
              )
              (activation_fns): ModuleList(
                (0): Softplus(beta=1, threshold=20)
                (1): Softplus(beta=1, threshold=20)
                (2): Softplus(beta=1, threshold=20)
                (3): Softplus(beta=1, threshold=20)
              )
            )
          )
        )
      )
    )
  )
  (time_change_func): time_change_func_id()
)
TimeChangedCNF(
  (sequential_flow): SequentialFlow(
    (chain): ModuleList(
      (0): CNF(
        (odefunc): RegularizedODEfunc(
          (odefunc): AugODEfunc(
            (diffeq): AugODEnet(
              (layers): ModuleList(
                (0): IgnoreLinear(
                  (_layer): Linear(in_features=2, out_features=8, bias=True)
                )
                (1): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=32, bias=True)
                )
                (2): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=32, bias=True)
                )
                (3): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=8, bias=True)
                )
                (4): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=1, bias=True)
                )
              )
              (activation_fns): ModuleList(
                (0): Softplus(beta=1, threshold=20)
                (1): Softplus(beta=1, threshold=20)
                (2): Softplus(beta=1, threshold=20)
                (3): Softplus(beta=1, threshold=20)
              )
            )
          )
        )
      )
    )
  )
  (time_change_func): time_change_func_id()
)
TimeChangedCNF(
  (sequential_flow): SequentialFlow(
    (chain): ModuleList(
      (0): CNF(
        (odefunc): RegularizedODEfunc(
          (odefunc): AugODEfunc(
            (diffeq): AugODEnet(
              (layers): ModuleList(
                (0): IgnoreLinear(
                  (_layer): Linear(in_features=2, out_features=8, bias=True)
                )
                (1): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=32, bias=True)
                )
                (2): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=32, bias=True)
                )
                (3): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=8, bias=True)
                )
                (4): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=1, bias=True)
                )
              )
              (activation_fns): ModuleList(
                (0): Softplus(beta=1, threshold=20)
                (1): Softplus(beta=1, threshold=20)
                (2): Softplus(beta=1, threshold=20)
                (3): Softplus(beta=1, threshold=20)
              )
            )
          )
        )
      )
    )
  )
  (time_change_func): time_change_func_id()
)
Number of trainable parameters: 1642
Number of trainable parameters: 1642
Number of trainable parameters: 1642
Number of trainable parameters: 1642
Number of trainable parameters: 1642
Number of trainable parameters: 1642
Number of trainable parameters: 1642
Iter 0000 | Time 0.4636(0.4636) | Bit/dim -1.0308(-1.0308) | Steps 0(0.00) | Grad Norm 0.2377(0.2377) | Total Time 1.00(1.00)
Iter 0000 | Time 0.4636(0.4636) | Bit/dim -1.0308(-1.0308) | Steps 0(0.00) | Grad Norm 0.2377(0.2377) | Total Time 1.00(1.00)
Iter 0000 | Time 0.4636(0.4636) | Bit/dim -1.0308(-1.0308) | Steps 0(0.00) | Grad Norm 0.2377(0.2377) | Total Time 1.00(1.00)
Iter 0000 | Time 0.4636(0.4636) | Bit/dim -1.0308(-1.0308) | Steps 0(0.00) | Grad Norm 0.2377(0.2377) | Total Time 1.00(1.00)
Iter 0000 | Time 0.4636(0.4636) | Bit/dim -1.0308(-1.0308) | Steps 0(0.00) | Grad Norm 0.2377(0.2377) | Total Time 1.00(1.00)
Iter 0000 | Time 0.4636(0.4636) | Bit/dim -1.0308(-1.0308) | Steps 0(0.00) | Grad Norm 0.2377(0.2377) | Total Time 1.00(1.00)
Iter 0000 | Time 0.4636(0.4636) | Bit/dim -1.0308(-1.0308) | Steps 0(0.00) | Grad Norm 0.2377(0.2377) | Total Time 1.00(1.00)
Iter 0010 | Time 0.2164(0.2186) | Bit/dim -0.9126(-0.9317) | Steps 0(0.00) | Grad Norm 0.0882(0.0768) | Total Time 1.00(1.00)
Iter 0010 | Time 0.2164(0.2186) | Bit/dim -0.9126(-0.9317) | Steps 0(0.00) | Grad Norm 0.0882(0.0768) | Total Time 1.00(1.00)
Iter 0010 | Time 0.2164(0.2186) | Bit/dim -0.9126(-0.9317) | Steps 0(0.00) | Grad Norm 0.0882(0.0768) | Total Time 1.00(1.00)
Iter 0010 | Time 0.2164(0.2186) | Bit/dim -0.9126(-0.9317) | Steps 0(0.00) | Grad Norm 0.0882(0.0768) | Total Time 1.00(1.00)
Iter 0010 | Time 0.2164(0.2186) | Bit/dim -0.9126(-0.9317) | Steps 0(0.00) | Grad Norm 0.0882(0.0768) | Total Time 1.00(1.00)
Iter 0010 | Time 0.2164(0.2186) | Bit/dim -0.9126(-0.9317) | Steps 0(0.00) | Grad Norm 0.0882(0.0768) | Total Time 1.00(1.00)
Iter 0010 | Time 0.2164(0.2186) | Bit/dim -0.9126(-0.9317) | Steps 0(0.00) | Grad Norm 0.0882(0.0768) | Total Time 1.00(1.00)
Iter 0020 | Time 0.1994(0.1974) | Bit/dim -0.8835(-0.9203) | Steps 0(0.00) | Grad Norm 0.0223(0.0303) | Total Time 1.00(1.00)
Iter 0020 | Time 0.1994(0.1974) | Bit/dim -0.8835(-0.9203) | Steps 0(0.00) | Grad Norm 0.0223(0.0303) | Total Time 1.00(1.00)
Iter 0020 | Time 0.1994(0.1974) | Bit/dim -0.8835(-0.9203) | Steps 0(0.00) | Grad Norm 0.0223(0.0303) | Total Time 1.00(1.00)
Iter 0020 | Time 0.1994(0.1974) | Bit/dim -0.8835(-0.9203) | Steps 0(0.00) | Grad Norm 0.0223(0.0303) | Total Time 1.00(1.00)
Iter 0020 | Time 0.1994(0.1974) | Bit/dim -0.8835(-0.9203) | Steps 0(0.00) | Grad Norm 0.0223(0.0303) | Total Time 1.00(1.00)
Iter 0020 | Time 0.1994(0.1974) | Bit/dim -0.8835(-0.9203) | Steps 0(0.00) | Grad Norm 0.0223(0.0303) | Total Time 1.00(1.00)
Iter 0020 | Time 0.1994(0.1974) | Bit/dim -0.8835(-0.9203) | Steps 0(0.00) | Grad Norm 0.0223(0.0303) | Total Time 1.00(1.00)
Iter 0030 | Time 0.2208(0.2174) | Bit/dim -0.9945(-0.9380) | Steps 0(0.00) | Grad Norm 0.0269(0.0325) | Total Time 1.00(1.00)
Iter 0030 | Time 0.2208(0.2174) | Bit/dim -0.9945(-0.9380) | Steps 0(0.00) | Grad Norm 0.0269(0.0325) | Total Time 1.00(1.00)
Iter 0030 | Time 0.2208(0.2174) | Bit/dim -0.9945(-0.9380) | Steps 0(0.00) | Grad Norm 0.0269(0.0325) | Total Time 1.00(1.00)
Iter 0030 | Time 0.2208(0.2174) | Bit/dim -0.9945(-0.9380) | Steps 0(0.00) | Grad Norm 0.0269(0.0325) | Total Time 1.00(1.00)
Iter 0030 | Time 0.2208(0.2174) | Bit/dim -0.9945(-0.9380) | Steps 0(0.00) | Grad Norm 0.0269(0.0325) | Total Time 1.00(1.00)
Iter 0030 | Time 0.2208(0.2174) | Bit/dim -0.9945(-0.9380) | Steps 0(0.00) | Grad Norm 0.0269(0.0325) | Total Time 1.00(1.00)
Iter 0030 | Time 0.2208(0.2174) | Bit/dim -0.9945(-0.9380) | Steps 0(0.00) | Grad Norm 0.0269(0.0325) | Total Time 1.00(1.00)
validating...
validating...
validating...
validating...
validating...
validating...
validating...
c:\users\naouf\documents\0-these\2-scripts\10-sto_norm_flow\4-tcnf_github\tcnf.py
# Copyright (c) 2019-present Royal Bank of Canada
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


#
import os.path as osp
import time
import os


import lib.utils as utils
import numpy as np
import torch
from lib.utils import optimizer_factory
import matplotlib.pyplot as plt

from bm_sequential import get_dataset
from tcnf_tools import build_TCNF
from tcnf_tools import run_tcnf_model as run_model, parse_arguments
from train_misc import (
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
)
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time

from torch.optim import lr_scheduler


RUNNINGAVE_PARAM = 0.7
torch.backends.cudnn.benchmark = True


def save_model(args, aug_model, time_change_func, optimizer, epoch, itr, save_path):
    """
    save CTFP model's checkpoint during training

    Parameters:
        args: the arguments from parse_arguments in ctfp_tools
        aug_model: the CTFP Model
        optimizer: optimizer of CTFP model
        epoch: training epoch
        itr: training iteration
        save_path: path to save the model
    """
    torch.save(
        {
            "args": args,
            "state_dict": aug_model.module.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else aug_model.state_dict(),
            "time_func_state_dict": time_change_func.state_dict()
            if torch.cuda.is_available() and not args.use_cpu
            else time_change_func.state_dict(),
            "optim_state_dict": optimizer.state_dict(),
            "last_epoch": epoch,
            "iter": itr,
        },
        save_path,
    )


if __name__ == "__main__":
    args = parse_arguments()
    
    ######## to remove
    args.time_change_MGN_N = 2
    args.time_change_MGN_K = 3
    ########
    
    args.l1_reg_loss = False
    args.alpha_l1_reg_loss = 1.
    
    args.l2_reg_loss = False
    args.alpha_l2_reg_loss = 1.
    ##### args  ######
    
    plot_phi_freq = 20
    bool_scheduler = False
    
    
    # logger
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"), filepath=os.path.abspath(__file__)
    )

    if args.layer_type == "blend":
        logger.info(
            "!! Setting time_length from None to 1.0 due to use of Blend layers."
        )
        args.time_length = 1.0
    logger.info(args)
    if not args.no_tb_log:
        from tensorboardX import SummaryWriter

        writer = SummaryWriter(osp.join(args.save, "tb_logs"))
        writer.add_text("args", str(args))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # get device
    if args.use_cpu:
        device = torch.device("cpu")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, val_loader = get_dataset(args)
    
    
    # create time-changed NF with time-change function
    regularization_fns, regularization_coeffs = create_regularization_fns(args)

    aug_model, time_change_func = build_TCNF(
        args,
        args.aug_size + args.effective_shape,
        regularization_fns=regularization_fns,
        time_change = args.time_change
    )
    
    
    set_cnf_options(args, aug_model)
    logger.info(aug_model)

    logger.info(
        "Number of trainable parameters: {}".format(count_parameters(aug_model))
    )

    # optimizer
    parameter_list = list(aug_model.parameters())+ list(time_change_func.parameters())
    optimizer, num_params = optimizer_factory(args, parameter_list)
    if bool_scheduler:
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=5)
    
    print("Num of Parameters: %d" % num_params)

    # restore parameters
        
    itr = 0
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)
        aug_model.load_state_dict(checkpt["state_dict"])
        time_change_func.load_state_dict(checkpt["time_func_state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)
        if "iter" in checkpt.keys():
            itr = checkpt["iter"]
        if "last_epoch" in checkpt.keys():
            args.begin_epoch = checkpt["last_epoch"] + 1

    if torch.cuda.is_available() and not args.use_cpu:
        aug_model = torch.nn.DataParallel(aug_model).cuda()
        time_change_func = time_change_func.cuda()

    # For visualization.

    time_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    loss_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    steps_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    grad_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)
    tt_meter = utils.RunningAverageMeter(RUNNINGAVE_PARAM)

    best_loss = float("inf")
    start_time = time.time()
    
    for epoch in range(args.begin_epoch, args.num_epochs + 1):
        aug_model.train()
        
        
        for temp_idx, x in enumerate(train_loader):
            ## x is a tuple of (values, times, stdv, masks)
            start = time.time()
            optimizer.zero_grad()

            # cast data and move to device
            x = map(cvt, x)
            values, times, vars, masks = x
            # compute loss
            loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)

            total_time = count_total_time(aug_model)
            ## Assume the base distribution be Brownian motion

            if regularization_coeffs:
                reg_states = get_regularization(aug_model, regularization_coeffs)
                reg_loss = sum(
                    reg_state * coeff
                    for reg_state, coeff in zip(reg_states, regularization_coeffs)
                    if coeff != 0
                )
                loss = loss + reg_loss
            
                        
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                aug_model.parameters(), args.max_grad_norm
            )
            optimizer.step()
            
        
            time_meter.update(time.time() - start)
            loss_meter.update(loss.item())
            steps_meter.update(count_nfe(aug_model))
            grad_meter.update(grad_norm)
            tt_meter.update(total_time)

            if not args.no_tb_log:
                writer.add_scalar("train/NLL", loss.cpu().data.item(), itr)

            if itr % args.log_freq == 0:
                log_message = (
                    "Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | "
                    "Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time "
                    "{:.2f}({:.2f})".format(
                        itr,
                        time_meter.val,
                        time_meter.avg,
                        loss_meter.val,
                        loss_meter.avg,
                        steps_meter.val,
                        steps_meter.avg,
                        grad_meter.val,
                        grad_meter.avg,
                        tt_meter.val,
                        tt_meter.avg,
                    )
                )
                
                
                if regularization_coeffs:
                    log_message = append_regularization_to_log(
                        log_message, regularization_fns, reg_states
                    )
                logger.info(log_message)
                                
                
            itr += 1
        
                    
        if epoch % args.val_freq == 0:
            
            
            
            with torch.no_grad():
                start = time.time()
                logger.info("validating...")
                losses = []
                num_observes = []
                aug_model.eval()
                for temp_idx, x in enumerate(val_loader):
                    ## x is a tuple of (values, times, stdv, masks)
                    start = time.time()

                    # cast data and move to device
                    x = map(cvt, x)
                    values, times, vars, masks = x
                    loss = run_model(args, aug_model, values, times, vars, masks, time_change_func)
                                            
                    # compute loss
                    losses.append(loss.data.cpu().numpy())
                    
                    num_observes.append(torch.sum(masks).data.cpu().numpy())

                loss = np.sum(np.array(losses) * np.array(num_observes)) / np.sum(
                    num_observes
                )
                
                
                if not args.no_tb_log:
                    writer.add_scalar("val/NLL", loss, epoch)
                logger.info(
                    "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}".format(
                        epoch, time.time() - start, loss
                    )
                )
                        
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_last.pth"),
                )
                save_model(
                    args,
                    aug_model,
                    time_change_func,
                    optimizer,
                    epoch,
                    itr,
                    os.path.join(args.save, "checkpt_%d.pth") % (epoch),
                )

                if loss < best_loss:
                    
                    num_best_epoch = epoch
                    best_loss = loss
                    save_model(
                        args,
                        aug_model,
                        time_change_func,
                        optimizer,
                        epoch,
                        itr,
                        os.path.join(args.save, "checkpt_best.pth"),
                    )
            
            if bool_scheduler:
                scheduler.step(loss)
            
    
    print("--- %s seconds ---" % (time.time() - start_time))
    
    
    
Namespace(data_path='data/gbm.pkl', use_cpu=False, num_workers=0, dims='8,32,32,8', aug_hidden_dims=None, aug_dim=0, strides='2,2,1,-2,-2', num_blocks=1, encoder='ode_rnn', conv=True, layer_type='ignore', divergence_fn='approximate', nonlinearity='softplus', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, input_size=1, aug_size=1, latent_size=10, rec_size=20, rec_layers=1, units=100, gru_units=100, num_iwae_samples=3, niwae_test=25, alpha=1e-06, time_length=1.0, train_T=True, aug_mapping=False, activation='exp', num_epochs=1000, batch_size=100, test_batch_size=50, lr=0.001, optimizer='adam', amsgrad=False, momentum=0.9, decoder_frequency=3, aggressive=False, weight_decay=0.0, batch_norm=False, residual=False, autoencode=False, rademacher=True, multiscale=False, parallel=False, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, time_penalty=0, max_grad_norm=10000000000.0, begin_epoch=1, resume=None, save='experiments\\time_change_NF', val_freq=1, log_freq=10, no_tb_log=False, test_split='test', time_change='id', time_change_MGN_K=3, time_change_MGN_N=2, l1_reg_loss=False, l2_reg_loss=False, alpha_l1_reg_loss=1.0, alpha_l2_reg_loss=1.0, effective_shape=1)
TimeChangedCNF(
  (sequential_flow): SequentialFlow(
    (chain): ModuleList(
      (0): CNF(
        (odefunc): RegularizedODEfunc(
          (odefunc): AugODEfunc(
            (diffeq): AugODEnet(
              (layers): ModuleList(
                (0): IgnoreLinear(
                  (_layer): Linear(in_features=2, out_features=8, bias=True)
                )
                (1): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=32, bias=True)
                )
                (2): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=32, bias=True)
                )
                (3): IgnoreLinear(
                  (_layer): Linear(in_features=32, out_features=8, bias=True)
                )
                (4): IgnoreLinear(
                  (_layer): Linear(in_features=8, out_features=1, bias=True)
                )
              )
              (activation_fns): ModuleList(
                (0): Softplus(beta=1, threshold=20)
                (1): Softplus(beta=1, threshold=20)
                (2): Softplus(beta=1, threshold=20)
                (3): Softplus(beta=1, threshold=20)
              )
            )
          )
        )
      )
    )
  )
  (time_change_func): time_change_func_id()
)
Number of trainable parameters: 1642
Iter 0000 | Time 1.6823(1.6823) | Bit/dim -0.8857(-0.8857) | Steps 0(0.00) | Grad Norm 0.5732(0.5732) | Total Time 1.00(1.00)
Iter 0010 | Time 0.1771(0.2213) | Bit/dim -0.9644(-0.9178) | Steps 0(0.00) | Grad Norm 0.2563(0.3244) | Total Time 1.00(1.00)
Iter 0020 | Time 0.1394(0.1626) | Bit/dim -1.0093(-0.9430) | Steps 0(0.00) | Grad Norm 0.0142(0.0609) | Total Time 1.00(1.00)
Iter 0030 | Time 0.2150(0.2052) | Bit/dim -0.8997(-0.9474) | Steps 0(0.00) | Grad Norm 0.0929(0.0865) | Total Time 1.00(1.00)
validating...
